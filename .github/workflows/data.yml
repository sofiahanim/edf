name: Data Pipeline

on:
  push:
    branches:
      - main  # Trigger the workflow on pushes to the main branch
  workflow_dispatch:  # Allow manual triggering
  schedule:
  - cron: '0 17 * * *' # 1:00 AM MALAYSIA TIME  # Schedule the workflow to run daily at 1:00 AM
   
permissions:
  contents: write  # Read and write permissions for repository contents

jobs:
  data_pipeline:
    runs-on: ubuntu-20.04

    steps:
      # Step 1: Check out the repository
      - name: Checkout code
        uses: actions/checkout@v2

      # Step 2: Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: "3.9"

      # Step 3: Install System-Level Dependencies
      - name: Install System-Level Dependencies
        run: |
          echo "üîÑ Installing system-level dependencies..."
          sudo apt-get update -y
          sudo apt-get install -y \
            build-essential \
            g++ \
            gcc \
            swig \
            cmake \
            python3-dev \
            python3-pip \
            libopenblas-dev \
            liblapack-dev \
            libatlas-base-dev \
            libffi-dev \
            libssl-dev \
            zlib1g-dev \
            libcurl4-openssl-dev \
            libhdf5-dev

      # Step 4: Upgrade Python Tools
      - name: Upgrade Python Tools
        run: |
          echo "üîÑ Upgrading pip, setuptools, and wheel..."
          python3 -m pip install --upgrade pip setuptools wheel

      # Step 5: Install Python Dependencies
      - name: Install Python Dependencies
        run: |
          echo "üîÑ Installing Python dependencies..."
          python3 -m pip install --only-binary=:all: --no-cache-dir \
            -r requirements_linux.txt --verbose

      # Step 6: Install auto-sklearn
      - name: Install auto-sklearn
        run: |
          echo "üîÑ Installing auto-sklearn..."
          python3 -m pip install auto-sklearn --verbose --no-cache-dir
        continue-on-error: true

      - name: Check Dependency Tree
        run: |
          python3 -m pip install pipdeptree
          pipdeptree


      # Step 4: Set AWS credentials (this will use the environment variables for AWS credentials)
      - name: Set AWS credentials
        run: |
          echo "AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}" >> $GITHUB_ENV
          echo "AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}" >> $GITHUB_ENV
          echo "AWS_DEFAULT_REGION=us-east-1" >> $GITHUB_ENV

      # Step 5: Verify file existence and content
      - name: Verify file existence and content
        run: |
          if [ -f "data/demand/2025.csv" ]; then
            echo "File exists:"
            ls -l data/demand/2025.csv
            if [ -s "data/demand/2025.csv" ]; then
              echo "CSV file has content:"
              cat data/demand/2025.csv
            else
              echo "CSV file is empty. Exiting pipeline."
              exit 1
            fi
          else
            echo "CSV file does not exist. Exiting pipeline."
            exit 1
          fi

      # Step 6: Run Python script to update CSV (if needed)
      - name: Run data pipeline script
        run: python3 scripts/pipe_data.py
        env:
          PYTHONUNBUFFERED: 1
          LOG_LEVEL: DEBUG

      # Step 7: Commit and push updated CSV
      - name: Commit and push updated CSV
        run: |
          git config --local user.name "GitHub Actions[bot]"
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git add --force data/demand/2025.csv
          git commit -m "Update CSV with new data" || echo "Nothing to commit."
          git push || echo "No changes pushed."

      # Step 9: Notify Success or Failure
      - name: Notify Completion
        if: always()
        run: |
          if [ "$GITHUB_JOB" == "success" ]; then
            echo "üéâ Workflow completed successfully!"
          else
            echo "‚ùå Workflow failed. Check logs for details."

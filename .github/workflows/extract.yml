name: Data Extraction

on:
  workflow_dispatch:  # Allows manual trigger
  schedule:
    - cron: '0 1 * * *'  # Runs at 1 AM UTC every day

jobs:
  data_extraction:
    runs-on: ubuntu-20.04  # Use Ubuntu 20.04 for compatibility

    steps:
      # Step 1: Checkout code
      - name: Checkout code
        uses: actions/checkout@v2

      # Step 2: Handle ~BROMIUM if it exists
      - name: Handle ~BROMIUM if it exists
        run: |
          if [ -e "/home/runner/~BROMIUM" ]; then  # Use full path
            echo "Skipping further steps because ~BROMIUM exists."
            exit 0  # Exit early, skipping remaining steps
          else
            echo "~BROMIUM does not exist, continuing normally"
          fi

      # Step 3: Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.11.10  # Ensure Python version matches requirements

      # Step 4: Install specific dependencies via pip
      - name: Install specific dependencies via pip
        run: |
          sudo apt-get update
          sudo apt-get install -y libdbus-1-dev  # Install dbus development files
          python3 -m pip install --upgrade pip
          python3 -m pip install Cython distro==1.7.0 dbus-python==1.2.18 pandas boto3 PyYAML==5.4.1
          # Add other dependencies as needed

      # Step 5: Install system dependencies
      - name: Install system dependencies
        run: |
          sudo apt-get install -y \
            libpq-dev libjemalloc-dev python3-gi gir1.2-gtk-3.0 \
            python3-secretstorage python3-keyring \
            python3-apt python3-pip ubuntu-advantage-tools ufw \
            unattended-upgrades


      # Step 6: Update pip and install Python dependencies
      - name: Install Python dependencies
        run: |
          python3 -m pip install pandas boto3 PyYAML==5.4.1
          # Add all other dependencies to the requirements file or install them here

      # Step 7: Run extraction script
      - name: Run extraction script
        env:
          REDSHIFT_REGION: ${{ secrets.REDSHIFT_REGION }}
          REDSHIFT_WORKGROUP: ${{ secrets.REDSHIFT_WORKGROUP }}
        run: python scripts/extract_data.py

      # Step 8: Archive logs
      - name: Archive logs
        uses: actions/upload-artifact@v3
        with:
          name: logs
          path: logs/

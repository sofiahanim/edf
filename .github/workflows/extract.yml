name: Data Extraction

on:
  workflow_dispatch:  # Allows manual trigger
  schedule:
    - cron: '0 1 * * *'  # Runs at 1 AM UTC every day

jobs:
  data_extraction:
    runs-on: ubuntu-20.04  # Use Ubuntu 20.04 for compatibility

    steps:
      # Step 1: Checkout code
      - name: Checkout code
        uses: actions/checkout@v2

      # Step 2: Handle ~BROMIUM if it exists
      - name: Handle ~BROMIUM if it exists
        run: |
          if [ -e "/home/runner/~BROMIUM" ]; then  # Use full path
            echo "Skipping further steps because ~BROMIUM exists."
            exit 0  # Exit early, skipping remaining steps
          else
            echo "~BROMIUM does not exist, continuing normally"
          fi

      # Step 3: Set up Python environment
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: 3.11.10  # Ensure Python version matches requirements

      # Step 4: Install specific dependencies via pip
      - name: Install specific dependencies via pip
        run: |
          sudo apt-get install -y libdbus-1-dev libpq-dev libjemalloc-dev python3-gi gir1.2-gtk-3.0 \
            python3-secretstorage python3-keyring python3-apt python3-pip ubuntu-advantage-tools ufw \
            unattended-upgrades libpython3-dev build-essential
          python3 -m pip install --upgrade pip
          python3 -m pip install -r requirement_linux.txt
          pip freeze > requirement_linux_updated.txt
          
      # Step 7: Run extraction script
      - name: Run extraction script
        env:
          REDSHIFT_REGION: ${{ secrets.REDSHIFT_REGION }}
          REDSHIFT_WORKGROUP: ${{ secrets.REDSHIFT_WORKGROUP }}
        run: python scripts/extract_data.py

      # Step 8: Archive logs
      - name: Archive logs
        uses: actions/upload-artifact@v3
        with:
          name: logs
          path: logs/
